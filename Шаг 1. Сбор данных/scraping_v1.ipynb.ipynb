{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg_uZkkBxyW5"
      },
      "outputs": [],
      "source": [
        "#pip install fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gdown\n",
        "import requests\n",
        "import time\n",
        "from fake_useragent import UserAgent\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "\n",
        "from multiprocessing import Pool"
      ],
      "metadata": {
        "id": "sOIvE7YcycSK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ru = pd.read_csv('ИПС/ru_domains.txt', delimiter=';', names=['name', 'registrator', 'creation_date', 'payed_due', 'delegated'])\n",
        "rf = pd.read_csv('ИПС/rf_domains.txt', delimiter=';', names=['name', 'registrator', 'creation_date', 'payed_due', 'delegated'])\n",
        "su = pd.read_csv('ИПС/su_domains.txt', delimiter=';', names=['name', 'registrator', 'creation_date', 'payed_due', 'delegated'])\n",
        "\n",
        "domains = pd.concat([ru, rf, su], ignore_index=True)"
      ],
      "metadata": {
        "id": "lXmCFTXZx-0a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fxlfE4JG43hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import cpu_count\n",
        "cpu_count() # Число параллельных процессов"
      ],
      "metadata": {
        "id": "-BSYx1rI5g7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b287d2-a9ba-4934-97e3-3e500ace0061"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Text\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Функция для скрапинга данных с одной ссылки\n",
        "def scrape_url(url):\n",
        "   try:\n",
        "    response = requests.get(f'http://{url}/', headers={'User-Agent': UserAgent().chrome}, timeout = 3)\n",
        "    html = response.content\n",
        "    soup = BeautifulSoup(html,'html.parser')\n",
        "\n",
        "    try:\n",
        "      des = soup.find(\"meta\",{\"name\":\"description\"})['content']\n",
        "    except:\n",
        "      des = ''\n",
        "    try:\n",
        "      tit = soup.find(\"meta\",{\"name\":\"title\"})['content']\n",
        "    except:\n",
        "      tit = ''\n",
        "    try:\n",
        "      tit2 = soup.find(\"title\").text\n",
        "    except:\n",
        "      tit2 = ''\n",
        "    try:\n",
        "      key = soup.find(\"meta\",{\"name\":\"keywords\"})['content']\n",
        "    except:\n",
        "      key = ''\n",
        "    try:\n",
        "      text = soup.body.get_text(' ', strip=True)\n",
        "    except: ''\n",
        "\n",
        "\n",
        "    pd.DataFrame({'url': [url], 'descr':[des], 'titl1': [tit], 'titl2': [tit2], 'keyw': [key], 'text': [text]}).to_csv('parsing.csv', mode='a', header=False)\n",
        "   except:\n",
        "    pd.DataFrame({'url': ['ERROR'], 'descr': ['ERROR'], 'titl1': ['ERROR'], 'titl2': ['ERROR'], 'keyw': ['ERROR'], 'text': ['ERROR']}).to_csv('parsing.csv', mode='a', header=False)\n",
        "\n",
        "# Параллельный скрапинг\n",
        "def scrape_parallel(url):\n",
        "    result = scrape_url(url)\n",
        "    return result"
      ],
      "metadata": {
        "id": "IW_GCeqp8-Sm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ПАРСИНГ"
      ],
      "metadata": {
        "id": "s5jEV6q96WPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'url': ['url'], 'descr':['descr'], 'titl1': ['titl1'], 'titl2': ['titl2'], 'keyw': ['keyw'], 'text': ['text']}).to_csv('parsing.csv', mode='a', header=False)"
      ],
      "metadata": {
        "id": "DJL9gKJe251r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_list = domains['name'][4200000:4500000] # Окно в 300к"
      ],
      "metadata": {
        "id": "9v5Xxqfs6Gzj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_processes = 96 # Число параллельных процессов\n",
        "\n",
        "with Pool(num_processes) as pool:\n",
        "    results = pool.map(scrape_parallel, url_list)"
      ],
      "metadata": {
        "id": "bYdE2Bx1x-_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}