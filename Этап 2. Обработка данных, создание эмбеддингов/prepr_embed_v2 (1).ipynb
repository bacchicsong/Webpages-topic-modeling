{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Если ОЗУ не хватает"
      ],
      "metadata": {
        "id": "HvyTJdlu7Yaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "id": "EHRJWD0n7WFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import fasttext\n",
        "from huggingface_hub import hf_hub_download\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import csv\n",
        "import sys"
      ],
      "metadata": {
        "id": "Nnwm4Vr2JOOj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # Подключение к диску с данными\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D8eJbYR3PKpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=\"facebook/fasttext-ru-vectors\", filename=\"model.bin\") # Загрузка модели\n",
        "model = fasttext.load_model(model_path)"
      ],
      "metadata": {
        "id": "0TUq4ZmBJJ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Обработка данных\n",
        "    \"\"\"\n",
        "    text = list(filter(str.isalpha, word_tokenize(text.lower()))) # Токенизация\n",
        "    text = list(lemmatizer.lemmatize(word) for word in text) # Лемматизация\n",
        "    text = list(word for word in text if word not in stop_words) # Удаление стоп-слов\n",
        "    return ' '.join(text)\n",
        "stop_words = set(stopwords.words('english') + stopwords.words('russian') + ['xn'])\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "8cLOvsb6JJ-C",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv.field_size_limit(3000000000000000000) # Технические параметры"
      ],
      "metadata": {
        "id": "uHNVaqwLJKDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame() # Запись обработанного текста\n",
        "lst = [] # Запись эмбеддингов\n",
        "errors = [] # Запись ошибок\n",
        "\n",
        "part = '4200_4500' # Часть названия файла\n",
        "\n",
        "with open(f'/content/drive/MyDrive/Парсинг ИПС_1/parsing_{part}.csv', newline='') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in tqdm(reader):\n",
        "    try:\n",
        "      clear_row = re.sub(r'\\s+', ' ', ' '.join(row).replace('\\n', ''))[2:]\n",
        "      if clear_row == 'ERROR ERROR ERROR ERROR ERROR ERROR' or len(clear_row) == 0 or clear_row == 'url descr titl1 titl2 keyw text': # Отсечение строк, о которых не получить информацию\n",
        "        continue\n",
        "      row_final = preprocess(clear_row) # Обработка данных\n",
        "      df = pd.concat((df, pd.DataFrame({'text': [row_final], 'url': [row[1]] }))) # Добавление текстовой информации ссылки в df\n",
        "      lst.append(model[row_final]) # Добавление эмбеддинга текстовой информации в lst\n",
        "    except:\n",
        "      errors.append(row)"
      ],
      "metadata": {
        "id": "_RNpKNtpJKFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(errors)"
      ],
      "metadata": {
        "id": "rIfkErRaJEPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(f'text_{part}.csv') # Сохранение текста\n",
        "np.save(f'embed_{part}.npy', lst) # Сохранение эмбеддинга"
      ],
      "metadata": {
        "id": "-5XihMwNJoZX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vy_0GhSS25CD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}