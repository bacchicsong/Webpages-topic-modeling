{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "id": "S7h1-WiiJQjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "from huggingface_hub import hf_hub_download\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import csv\n",
        "import sys"
      ],
      "metadata": {
        "id": "Nnwm4Vr2JOOj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=\"facebook/fasttext-ru-vectors\", filename=\"model.bin\") # Загрузка модели\n",
        "model = fasttext.load_model(model_path)"
      ],
      "metadata": {
        "id": "0TUq4ZmBJJ7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Обработка данных\n",
        "    \"\"\"\n",
        "    text = list(filter(str.isalpha, word_tokenize(text.lower()))) # Токенизация\n",
        "    text = list(lemmatizer.lemmatize(word) for word in text) # Лемматизация\n",
        "    text = list(word for word in text if word not in stop_words) # Удаление стоп-слов\n",
        "    return ' '.join(text)\n",
        "stop_words = set(stopwords.words('english') + stopwords.words('russian') + ['xn'])\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "8cLOvsb6JJ-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame() # Запись обработанного текста\n",
        "lst = [] # Запись эмбеддингов"
      ],
      "metadata": {
        "id": "SeXueuvLJKAj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv.field_size_limit(3000000000000000000) # Технические параметры"
      ],
      "metadata": {
        "id": "uHNVaqwLJKDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/Test/parsing.csv', newline='') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in tqdm(reader):\n",
        "    clear_row = re.sub(r'\\s+', ' ', ' '.join(row).replace('\\n', ''))[2:]\n",
        "    if clear_row == 'ERROR ERROR ERROR ERROR ERROR ERROR' or len(clear_row) == 0: # Отсечение строк, о которых не получить информацию\n",
        "      continue\n",
        "    row = preprocess(clear_row) # Обработка данных\n",
        "    df = pd.concat((df, pd.DataFrame({'text': [row]}))) # Добавление текстовой информации ссылки в df\n",
        "    lst.append(model[row]) # Добавление эмбеддинга текстовой информации в lst"
      ],
      "metadata": {
        "id": "_RNpKNtpJKFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('text.csv') # Сохранение текста\n",
        "np.save('embed.npy', lst) # Сохранение эмбеддинга"
      ],
      "metadata": {
        "id": "-5XihMwNJoZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "475HYAs-Jobj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZWRYa-VJodz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-YiLbMDnJogK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}